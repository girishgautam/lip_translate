{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "loaded_data = np.load('/home/girishj/code/girishgautam/lip_translate/test_data.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npz file without immediately trying to access its items\n",
    "# Print out all available keys in the .npz file\n",
    "print(\"Keys in the .npz file:\", loaded_data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data['arr_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f551e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(loaded_data['arr_0'],axis=-1)[:-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b3f3b",
   "metadata": {},
   "source": [
    "1. get all code from girish github for laod model.\n",
    "2. need to expand dims on th array.\n",
    "2. need to modify code to take chunks of 75 and drop the remainder of the frames.\n",
    "2. get uvicorn running prediction on some test video that cat sent.\n",
    "3. get prediction of matilda video.\n",
    "\n",
    "\n",
    "\n",
    "4. gif?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a03bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ffca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55307079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/girishj/code/girishgautam/lip_translate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbf4acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lip_translate.api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc5e1e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "5\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "10\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "15\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "20\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "30 70\n",
      "True\n",
      "25\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for lip_translate/model_mathilda_2000_12mar/checkpoint_epoch-100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames))\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# response = requests.post(\"http://127.0.0.1:8000/predict\", json=json.dumps(frames))\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# print(response)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# if response.ok:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     frames = []\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     print(response)\u001b[39;00m\n",
      "File \u001b[0;32m~/code/girishgautam/lip_translate/lip_translate/api.py:77\u001b[0m, in \u001b[0;36mtest_function\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Convert chunk to NumPy array for processing\u001b[39;00m\n\u001b[1;32m     76\u001b[0m vid_frames_grey \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(np\u001b[38;5;241m.\u001b[39marray(chunk), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28mprint\u001b[39m(vid_frames_grey\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Process the chunk with your model here\u001b[39;00m\n\u001b[1;32m     80\u001b[0m predicted_text \u001b[38;5;241m=\u001b[39m load_checkpoints(vid_frames_grey)  \u001b[38;5;66;03m# Placeholder for model processing\u001b[39;00m\n",
      "File \u001b[0;32m~/code/girishgautam/lip_translate/lip_translate/load_checkpoints.py:14\u001b[0m, in \u001b[0;36mload_checkpoints\u001b[0;34m(data_frames)\u001b[0m\n\u001b[1;32m     11\u001b[0m epoch_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# for example, to load from checkpoint_epoch-06\u001b[39;00m\n\u001b[1;32m     12\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint_epoch-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_number\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m predicted_text \u001b[38;5;241m=\u001b[39m predict_video(model, data_frames)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_text\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lip_translate_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lip_translate_env/lib/python3.10/site-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     27\u001b[0m error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot found in checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find any \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatching files for\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[0;32m---> 31\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSliced checkpoints are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mUnimplementedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for lip_translate/model_mathilda_2000_12mar/checkpoint_epoch-100"
     ]
    }
   ],
   "source": [
    "loaded_data = np.load('/home/girishj/code/girishgautam/lip_translate/mathilda_test.npz', allow_pickle=True)\n",
    "success = True\n",
    "i = 0\n",
    "frames = []\n",
    "\n",
    "while success:\n",
    "\n",
    "    loaded_frame = loaded_data['arr_0'][i].tolist()\n",
    "    print(len(loaded_frame), len((loaded_frame)[0]))\n",
    "    if loaded_frame is not None:\n",
    "        success = True\n",
    "        print(success)\n",
    "        frames.append(loaded_frame)\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            print(len(frames))\n",
    "            # response = requests.post(\"http://127.0.0.1:8000/predict\", json=json.dumps(frames))\n",
    "            test_function(frames)\n",
    "            # print(response)\n",
    "        # if response.ok:\n",
    "        #     frames = []\n",
    "        # else:\n",
    "        #     print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfa951",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d925139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab97c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vidcap = cv2.VideoCapture(vid)\n",
    "# success = True\n",
    "# i = 0\n",
    "\n",
    "# # st.write('---- video capturing ----')\n",
    "\n",
    "# frames = []\n",
    "# while success:\n",
    "#         # while vidcap.isOpened():\n",
    "#     # while i <= 10:\n",
    "#     success, frame = vidcap.read()\n",
    "#     if frame is not None:\n",
    "#         img = im.fromarray(frame).convert('L')\n",
    "#         lips = lip_detect(np.array(img))\n",
    "#         frames.append(lips.tolist())\n",
    "#         i += 1\n",
    "#         if i % 10 == 0:\n",
    "#             response = requests.post(\"https://lip-reader-docker-zn34um6luq-nw.a.run.app/send_frames/\", json=json.dumps(frames))\n",
    "#             if response.ok:\n",
    "#                 frames = []\n",
    "#             else:\n",
    "#                 st.write(response)\n",
    "#             st.write(f'---- frame {i} complete {str(response)} ----')\n",
    "\n",
    "# vidcap.release()\n",
    "\n",
    "# response = requests.post(\"https://lip-reader-docker-zn34um6luq-nw.a.run.app/send_frames/\", json=json.dumps(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5c43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ff051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
